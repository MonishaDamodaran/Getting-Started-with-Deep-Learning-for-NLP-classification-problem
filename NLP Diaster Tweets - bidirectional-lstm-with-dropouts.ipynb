{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820cfc82",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-17T18:18:05.191526Z",
     "iopub.status.busy": "2022-03-17T18:18:05.189978Z",
     "iopub.status.idle": "2022-03-17T18:18:18.173122Z",
     "shell.execute_reply": "2022-03-17T18:18:18.173547Z",
     "shell.execute_reply.started": "2022-03-17T17:20:06.262273Z"
    },
    "papermill": {
     "duration": 13.008885,
     "end_time": "2022-03-17T18:18:18.173800",
     "exception": false,
     "start_time": "2022-03-17T18:18:05.164915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/nlp-getting-started/sample_submission.csv\n",
      "../input/nlp-getting-started/train.csv\n",
      "../input/nlp-getting-started/test.csv\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_rm_punctuation</th>\n",
       "      <th>text_rm_stopwords</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "      <td>deeds reason #earthquake may allah forgive us</td>\n",
       "      <td>deed reason earthquake may allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "      <td>forest near la ronge sask. canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>residents asked 'shelter place' notified offic...</td>\n",
       "      <td>resident ask shelter place notify officers. ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>13,000 receive wildfires evacuation order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "      <td>got sent photo ruby #alaska smoke #wildfires p...</td>\n",
       "      <td>sent photo ruby alaska smoke wildfires pour sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>two giant cranes holding bridge collapse nearb...</td>\n",
       "      <td>two giant crane hold bridge collapse nearby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @thetawniest the out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>@aria_ahrary @thetawniest the out of control w...</td>\n",
       "      <td>@aria_ahrary @thetawniest control wild fires c...</td>\n",
       "      <td>aria_ahrary thetawniest control wild even nort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m1.94 [01:04 utc]?5km s of volcano hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>m1.94 [01:04 utc]?5km s of volcano hawaii. htt...</td>\n",
       "      <td>m1.94 [01:04 utc]?5km volcano hawaii. http://t...</td>\n",
       "      <td>m1.94 0104 utc5km volcano hawaii.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>police investigating after an e-bike collided ...</td>\n",
       "      <td>police investigating e-bike collided car littl...</td>\n",
       "      <td>investigate e-bike collide car little portugal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the latest: more homes razed by northern calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>the latest: more homes razed by northern calif...</td>\n",
       "      <td>latest: homes razed northern california wildfi...</td>\n",
       "      <td>latest raze northern wildfire abc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     our deeds are the reason of this #earthquake m...       1   \n",
       "1                forest fire near la ronge sask. canada       1   \n",
       "2     all residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     just got sent this photo from ruby #alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @thetawniest the out of control w...       1   \n",
       "7610  m1.94 [01:04 utc]?5km s of volcano hawaii. htt...       1   \n",
       "7611  police investigating after an e-bike collided ...       1   \n",
       "7612  the latest: more homes razed by northern calif...       1   \n",
       "\n",
       "                                    text_rm_punctuation  \\\n",
       "0     our deeds are the reason of this #earthquake m...   \n",
       "1                forest fire near la ronge sask. canada   \n",
       "2     all residents asked to 'shelter in place' are ...   \n",
       "3     13,000 people receive #wildfires evacuation or...   \n",
       "4     just got sent this photo from ruby #alaska as ...   \n",
       "...                                                 ...   \n",
       "7608  two giant cranes holding a bridge collapse int...   \n",
       "7609  @aria_ahrary @thetawniest the out of control w...   \n",
       "7610  m1.94 [01:04 utc]?5km s of volcano hawaii. htt...   \n",
       "7611  police investigating after an e-bike collided ...   \n",
       "7612  the latest: more homes razed by northern calif...   \n",
       "\n",
       "                                      text_rm_stopwords  \\\n",
       "0         deeds reason #earthquake may allah forgive us   \n",
       "1                forest fire near la ronge sask. canada   \n",
       "2     residents asked 'shelter place' notified offic...   \n",
       "3     13,000 people receive #wildfires evacuation or...   \n",
       "4     got sent photo ruby #alaska smoke #wildfires p...   \n",
       "...                                                 ...   \n",
       "7608  two giant cranes holding bridge collapse nearb...   \n",
       "7609  @aria_ahrary @thetawniest control wild fires c...   \n",
       "7610  m1.94 [01:04 utc]?5km volcano hawaii. http://t...   \n",
       "7611  police investigating e-bike collided car littl...   \n",
       "7612  latest: homes razed northern california wildfi...   \n",
       "\n",
       "                                        lemmatized_text  \n",
       "0              deed reason earthquake may allah forgive  \n",
       "1                     forest near la ronge sask. canada  \n",
       "2     resident ask shelter place notify officers. ev...  \n",
       "3             13,000 receive wildfires evacuation order  \n",
       "4     sent photo ruby alaska smoke wildfires pour sc...  \n",
       "...                                                 ...  \n",
       "7608        two giant crane hold bridge collapse nearby  \n",
       "7609  aria_ahrary thetawniest control wild even nort...  \n",
       "7610                  m1.94 0104 utc5km volcano hawaii.  \n",
       "7611  investigate e-bike collide car little portugal...  \n",
       "7612                  latest raze northern wildfire abc  \n",
       "\n",
       "[7613 rows x 8 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "for dir_, _, file in os.walk(r'../input/nlp-getting-started'):\n",
    "    for file_name in file:\n",
    "        print(os.path.join(dir_, file_name))\n",
    "        \n",
    "        \n",
    "train = pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\n",
    "\n",
    "\n",
    "# --------------------------------- Text Preprocessing -------------------------------------------\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {'N': wordnet.NOUN, 'V':wordnet.VERB, 'J':wordnet.ADJ, 'R':wordnet.ADV}\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return ' '.join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "#     url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = re.sub(r'https?://[\\w]*[\\.\\\\\\/\\w\\d]*', '', text)\n",
    "#     text = url.sub(r'', text)\n",
    "    text = re.sub(r'[\\x80-\\xFF]+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_symbols(text):\n",
    "    symbols = re.compile(r'[@#?\\'\\\":\\[\\]]*')\n",
    "    return re.sub(r'[@#?\\'\\\":\\[\\]]*', '', text)\n",
    "\n",
    "def create_vocab(data):\n",
    "    vocab = []\n",
    "    for words in data['lemmatized_text'].values:\n",
    "        for word in words.split():\n",
    "            vocab.append(word)\n",
    "    vocab_dict = Counter(vocab)\n",
    "    freq_words = [word for word, counts in vocab_dict.most_common(50)]\n",
    "    rare_word = [word for word, counts in vocab_dict.most_common()[:-50:-1]]\n",
    "    return {'vocab_dict':vocab_dict\n",
    "           ,'freq_words':freq_words\n",
    "           ,'rare_word': rare_word}\n",
    "\n",
    "# freq_words = []\n",
    "# for word, counts in vocab_dict.most_common(50):\n",
    "#     freq_words.append(word)\n",
    "\n",
    "# rare_word = []\n",
    "# rare = vocab_dict.most_common()[:-50:-1]\n",
    "# for word, counts in rare:\n",
    "#     rare_word.append(word)\n",
    "\n",
    "\n",
    "def frequent_words(sentence, freq):\n",
    "    \n",
    "    text = ' '.join([word for word in sentence.split() if word not in freq])\n",
    "    return text\n",
    "    \n",
    "def rare_words_removal(sentence, rare):\n",
    "    text = ' '.join([word for word in sentence.split() if word not in rare])\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_preprocessing(train):\n",
    "    data = train.copy()\n",
    "    data['text'] = data['text'].str.lower()\n",
    "    data['text_rm_punctuation'] = data['text'].apply(lambda x: remove_punctuation(x))\n",
    "    data['text_rm_stopwords'] = data['text_rm_punctuation'].apply(lambda x: remove_stopwords(x))\n",
    "    data['lemmatized_text'] = data['text_rm_stopwords'].apply(lambda x: lemmatize_words(x))\n",
    "    data['lemmatized_text'] = data['lemmatized_text'].apply(lambda x: remove_html_tags(x))\n",
    "    data['lemmatized_text'] = data['lemmatized_text'].apply(lambda x: remove_urls(x))\n",
    "    data['lemmatized_text'] = data['lemmatized_text'].apply(lambda x: remove_symbols(x))\n",
    "   \n",
    "    vocab = []\n",
    "    for words in data['lemmatized_text'].values:\n",
    "        for word in words.split():\n",
    "            vocab.append(word)\n",
    "    vocab_dict = Counter(vocab)\n",
    "    freq_words = [word for word, counts in vocab_dict.most_common(50)]\n",
    "    rare_word = [word for word, counts in vocab_dict.most_common()[:-50:-1]]\n",
    "    freq_rare = {'vocab_dict':vocab_dict\n",
    "           ,'freq_words':freq_words\n",
    "           ,'rare': rare_word}\n",
    "\n",
    "\n",
    "    freq_words  = freq_rare['freq_words']\n",
    "    rare_words =  freq_rare['rare']\n",
    "    \n",
    "    print(type(rare_words))\n",
    "    \n",
    "    data['lemmatized_text'] = data['lemmatized_text'].apply(lambda x: rare_words_removal(x, rare_words))\n",
    "    data['lemmatized_text'] = data['lemmatized_text'].apply(lambda x: frequent_words(x, freq_words))\n",
    "    \n",
    "    return data\n",
    "    \n",
    "    \n",
    "    \n",
    "train_data  = text_preprocessing(train)\n",
    "test_data = text_preprocessing(test)\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b685d58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:18:18.246564Z",
     "iopub.status.busy": "2022-03-17T18:18:18.245739Z",
     "iopub.status.idle": "2022-03-17T18:18:18.294559Z",
     "shell.execute_reply": "2022-03-17T18:18:18.295504Z",
     "shell.execute_reply.started": "2022-03-17T17:20:22.645232Z"
    },
    "papermill": {
     "duration": 0.097448,
     "end_time": "2022-03-17T18:18:18.295744",
     "exception": false,
     "start_time": "2022-03-17T18:18:18.198296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for words in train_data['lemmatized_text'].values:\n",
    "    for word in words.split():\n",
    "        vocab.append(word)\n",
    "vocab_dict = Counter(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a54d2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:18:18.401120Z",
     "iopub.status.busy": "2022-03-17T18:18:18.400319Z",
     "iopub.status.idle": "2022-03-17T18:18:18.405013Z",
     "shell.execute_reply": "2022-03-17T18:18:18.405794Z",
     "shell.execute_reply.started": "2022-03-17T17:20:22.677739Z"
    },
    "papermill": {
     "duration": 0.055697,
     "end_time": "2022-03-17T18:18:18.405986",
     "exception": false,
     "start_time": "2022-03-17T18:18:18.350289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19046"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_dict.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a0e9dfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:18:18.492428Z",
     "iopub.status.busy": "2022-03-17T18:18:18.491834Z",
     "iopub.status.idle": "2022-03-17T18:18:19.006629Z",
     "shell.execute_reply": "2022-03-17T18:18:19.005965Z",
     "shell.execute_reply.started": "2022-03-17T17:20:22.710232Z"
    },
    "papermill": {
     "duration": 0.562843,
     "end_time": "2022-03-17T18:18:19.006774",
     "exception": false,
     "start_time": "2022-03-17T18:18:18.443931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unfrequent = []\n",
    "for i in range(99):\n",
    "    unfrequent.append(vocab_dict.most_common()[-100:-1][i][0])\n",
    "# vocab_dict.most_common()[-100:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "043bebcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:18:19.056087Z",
     "iopub.status.busy": "2022-03-17T18:18:19.055394Z",
     "iopub.status.idle": "2022-03-17T18:18:19.058016Z",
     "shell.execute_reply": "2022-03-17T18:18:19.058519Z",
     "shell.execute_reply.started": "2022-03-17T17:20:23.170059Z"
    },
    "papermill": {
     "duration": 0.02939,
     "end_time": "2022-03-17T18:18:19.058639",
     "exception": false,
     "start_time": "2022-03-17T18:18:19.029249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'metal) slower squeaver hangin watchin septic captainn_morgan friggin destiel (read description) gazette pedals. aqua... memenaar kindof restrospect _pokemoncards_ icequeenfroslas artectura pop2015 n36 florence 1979 gimp newave progressives. live-streaming girlthatsrio worse. georgefoster72 edmund fitzgerald blockage woodward northbound davison m.s. shoalstraffic blinker -))) misscharleywebb indeed! by! greer amazondeals skylanders 4.53% ($0.45) $9.49 $9.94 ralph titortau lynch hi-larious realtime. fact-checking ombudsmanship. raineishida lol...im nervous takeaway magnificent (vice news) victims janeenorman probability kuala lumpur washed plot! *rolling eyes* ajabrown ministersays buzzfeed first-ever 777 239 najibrazak malaysiaairlines yahoonewsdigest julian_lage grantgordy rossmartin7 pastie industrial stare (costing $100 apiece) liv oliviaapalmerr thatswhatfriendsarefor audi land. kunstler residualincome'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfrequent_strings = ' '.join([i for i in unfrequent])\n",
    "unfrequent_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52605a77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:18:19.109127Z",
     "iopub.status.busy": "2022-03-17T18:18:19.108592Z",
     "iopub.status.idle": "2022-03-17T18:21:56.423698Z",
     "shell.execute_reply": "2022-03-17T18:21:56.423219Z",
     "shell.execute_reply.started": "2022-03-17T17:20:23.180302Z"
    },
    "papermill": {
     "duration": 217.343303,
     "end_time": "2022-03-17T18:21:56.423832",
     "exception": false,
     "start_time": "2022-03-17T18:18:19.080529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [02:43, 5.28MB/s]                           \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 399999/400000 [00:31<00:00, 12555.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400000, 200])\n",
      "219\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "glove_emb = torchtext.vocab.GloVe(name = '6B', dim = 200)\n",
    "\n",
    "print(glove_emb.vectors.size())\n",
    "print(glove_emb.stoi['good'])\n",
    "print(glove_emb.itos[219])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "320cb4fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:21:56.934789Z",
     "iopub.status.busy": "2022-03-17T18:21:56.934162Z",
     "iopub.status.idle": "2022-03-17T18:21:56.939943Z",
     "shell.execute_reply": "2022-03-17T18:21:56.939534Z",
     "shell.execute_reply.started": "2022-03-17T17:24:14.217630Z"
    },
    "papermill": {
     "duration": 0.276267,
     "end_time": "2022-03-17T18:21:56.940052",
     "exception": false,
     "start_time": "2022-03-17T18:21:56.663785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed_no = 33\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_no)\n",
    "random.seed(seed_no)\n",
    "np.random.seed(seed_no)\n",
    "torch.manual_seed(seed_no)\n",
    "torch.cuda.manual_seed_all(seed_no)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e783f94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:21:57.426388Z",
     "iopub.status.busy": "2022-03-17T18:21:57.424779Z",
     "iopub.status.idle": "2022-03-17T18:21:57.426950Z",
     "shell.execute_reply": "2022-03-17T18:21:57.427375Z",
     "shell.execute_reply.started": "2022-03-17T17:24:14.231322Z"
    },
    "papermill": {
     "duration": 0.248524,
     "end_time": "2022-03-17T18:21:57.427510",
     "exception": false,
     "start_time": "2022-03-17T18:21:57.178986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the indices of the lemmatized text column\n",
    "\n",
    "def glove_index(text, glove_emb):\n",
    "    idx = [glove_emb.stoi[word] for word in text.split() if word in glove_emb.stoi]\n",
    "    return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4dbce2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:21:57.966256Z",
     "iopub.status.busy": "2022-03-17T18:21:57.921982Z",
     "iopub.status.idle": "2022-03-17T18:21:57.973009Z",
     "shell.execute_reply": "2022-03-17T18:21:57.972541Z",
     "shell.execute_reply.started": "2022-03-17T17:24:14.240647Z"
    },
    "papermill": {
     "duration": 0.308155,
     "end_time": "2022-03-17T18:21:57.973142",
     "exception": false,
     "start_time": "2022-03-17T18:21:57.664987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data['glove_index'] = train_data['lemmatized_text'].apply(lambda x: glove_index(x, glove_emb))\n",
    "test_data['glove_index'] = test_data['lemmatized_text'].apply(lambda x: glove_index(x, glove_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b9406a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:21:58.639525Z",
     "iopub.status.busy": "2022-03-17T18:21:58.638720Z",
     "iopub.status.idle": "2022-03-17T18:22:02.928336Z",
     "shell.execute_reply": "2022-03-17T18:22:02.927817Z",
     "shell.execute_reply.started": "2022-03-17T17:24:14.312296Z"
    },
    "papermill": {
     "duration": 4.710232,
     "end_time": "2022-03-17T18:22:02.928482",
     "exception": false,
     "start_time": "2022-03-17T18:21:58.218250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from keras.preprocessing import text, sequence \n",
    "MAX_LEN = 150\n",
    "\n",
    "class TextDatset(data.Dataset):\n",
    "    def __init__(self, text, lens, text_id, y = None):\n",
    "        self.text = text\n",
    "        self.lens = lens\n",
    "        self.text_id = text_id\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lens)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.text[idx], self.lens[idx], self.text_id[idx]\n",
    "        return self.text[idx], self.lens[idx], self.y[idx], self.text_id[idx]\n",
    "    \n",
    "\n",
    "class Collator(object):\n",
    "    def __init__(self, test = False, percentile = 100):\n",
    "        self.test = test\n",
    "        self.percentile = percentile\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        global MAX_LEN\n",
    "        \n",
    "        if self.test:\n",
    "            texts, lens, text_id = zip(*batch)\n",
    "        else:\n",
    "            texts, lens, target, text_id = zip(*batch)\n",
    "            \n",
    "        lens = np.array(lens)\n",
    "        \n",
    "#         max_len = min(int(np.percentile(lens, self.percentile)), MAX_LEN)\n",
    "        max_len = max(lens)\n",
    "        \n",
    "#         print(f'the sequence length is {max_len}')\n",
    "        texts = torch.tensor(sequence.pad_sequences(texts, maxlen = max_len), dtype = torch.long)\n",
    "        \n",
    "        if self.test:\n",
    "            return texts, text_id\n",
    "        \n",
    "        return texts, torch.tensor(target, dtype = torch.long), text_id\n",
    "#         return \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9447e16b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:22:03.413550Z",
     "iopub.status.busy": "2022-03-17T18:22:03.412697Z",
     "iopub.status.idle": "2022-03-17T18:22:03.414525Z",
     "shell.execute_reply": "2022-03-17T18:22:03.414970Z",
     "shell.execute_reply.started": "2022-03-17T17:24:19.963106Z"
    },
    "papermill": {
     "duration": 0.245978,
     "end_time": "2022-03-17T18:22:03.415099",
     "exception": false,
     "start_time": "2022-03-17T18:22:03.169121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_size = 200\n",
    "num_layers = 3\n",
    "hidden_size = 256\n",
    "num_classes = 2\n",
    "batch_size = 64\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f69cfe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:22:03.899055Z",
     "iopub.status.busy": "2022-03-17T18:22:03.898285Z",
     "iopub.status.idle": "2022-03-17T18:22:03.900651Z",
     "shell.execute_reply": "2022-03-17T18:22:03.900244Z",
     "shell.execute_reply.started": "2022-03-17T17:24:19.973135Z"
    },
    "papermill": {
     "duration": 0.246772,
     "end_time": "2022-03-17T18:22:03.900760",
     "exception": false,
     "start_time": "2022-03-17T18:22:03.653988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(pred, target):\n",
    "    pred = pred.max(1)[1]\n",
    "    correct = (pred == target).long().sum().item()\n",
    "    acc = correct / len(target)\n",
    "#     return acc, correct, len(target)\n",
    "    return acc\n",
    "\n",
    "# accuracy(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c95f4f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:22:04.397269Z",
     "iopub.status.busy": "2022-03-17T18:22:04.396526Z",
     "iopub.status.idle": "2022-03-17T18:22:04.398678Z",
     "shell.execute_reply": "2022-03-17T18:22:04.399108Z",
     "shell.execute_reply.started": "2022-03-17T18:14:14.393075Z"
    },
    "papermill": {
     "duration": 0.252659,
     "end_time": "2022-03-17T18:22:04.399264",
     "exception": false,
     "start_time": "2022-03-17T18:22:04.146605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, features, batch_size):\n",
    "    \n",
    "        # features and sequences are nothing but the input size and sequence length\n",
    "        super(Attention, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.features = features\n",
    "#         self.sequence = sequence\n",
    "#         self.bias = True\n",
    "        \n",
    "        self.W = torch.zeros(features, 1)\n",
    "        \n",
    "#         if bias:\n",
    "#             self.b = torch.zeros(sequence , 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x, sequence, bias = True, msg = False):\n",
    "        \n",
    "        \n",
    "#         print(x.contiguous().view(-1, self.features).shape, self.W.shape)\n",
    "        eij = torch.mm(x.contiguous().view(-1, self.features), self.W).view(-1, sequence)\n",
    "        \n",
    "        if msg:\n",
    "            print(f'---The shape of the input given to attention layer {x.shape} \\n', )\n",
    "            print(f'---The shape of the input after multiplying it with weights {eij.shape} \\n')\n",
    "        if bias:\n",
    "            eij = eij + torch.zeros(sequence)\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if msg: \n",
    "            print(f'---The shape of the input after tanh and exponentiating {a.shape}')\n",
    "        a = a / (torch.sum(a, 1).view(x.size(0), -1) + 1e-10)\n",
    "        \n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        if msg:\n",
    "            print(f'---The shape of the attention {a.shape} \\n')\n",
    "            print(f'---The shape of the final output from the attention layer {weighted_input.shape} and after summing {torch.sum(weighted_input, 1).shape} \\n')\n",
    "        return torch.sum(weighted_input, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d61d6925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:22:04.898385Z",
     "iopub.status.busy": "2022-03-17T18:22:04.897560Z",
     "iopub.status.idle": "2022-03-17T18:22:04.906117Z",
     "shell.execute_reply": "2022-03-17T18:22:04.905721Z",
     "shell.execute_reply.started": "2022-03-17T18:14:25.823827Z"
    },
    "papermill": {
     "duration": 0.261087,
     "end_time": "2022-03-17T18:22:04.906249",
     "exception": false,
     "start_time": "2022-03-17T18:22:04.645162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class textRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, batch_size, model_type = 'RNN'):\n",
    "        super(textRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.model_type = model_type\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove_emb.vectors, freeze = True)\n",
    "        \n",
    "        if model_type == 'RNN':\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, bidirectional = True, batch_first = True)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, bidirectional = True, batch_first = True)\n",
    "         \n",
    "        self.attention = Attention(2*hidden_size, batch_size)\n",
    "#         self.ff = nn.Linear(2*hidden_size, 50)\n",
    "#         self.ff1 = nn.Linear(50, num_classes)\n",
    "        self.ff = nn.Linear(2*hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, msg = True):\n",
    "        \n",
    "        h0 = torch.zeros(2*num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(2*num_layers, x.size(0), self.hidden_size)\n",
    "        #x should be of shape (batch_size, L, input_size)\n",
    "        \n",
    "        if msg:\n",
    "            print(f'The dimension of hidden layers and inputs before passing to the model \\n')\n",
    "            print(f'...... \\n')\n",
    "            print(f'---Input shape {x.shape} \\n')\n",
    "            print(f'---hidden layer shape {h0.shape} \\n')\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "#         x = x.permute(1, 0, 2)\n",
    "        if self.model_type == 'RNN':\n",
    "            output, hidden_state = self.rnn(x, h0)\n",
    "        else:\n",
    "            output, (hidden_state, cell_state) = self.rnn(x, (h0, c0))\n",
    "        \n",
    "        #output is of shape (batch_size, L, hidden_size)\n",
    "        #final hidden state of size (num_layers, batch_size, hidden_size)\n",
    "        \n",
    "       \n",
    "        #out = self.ff(final_hidden_state.view(x.size(1), -1))\n",
    "        att = self.attention(output, output.size(1), msg = False)\n",
    "        out = self.ff(att)\n",
    "#         out = self.dropout(self.ff(output[:, -1, :]))\n",
    "        out = (self.dropout(out))\n",
    "        \n",
    "        \n",
    "        \n",
    "        if msg:\n",
    "            print(f'The dimension of final hidden state and output state after passing to the model \\n')\n",
    "            print(f'....... \\n')\n",
    "            print(f'---Input shape {x.shape} \\n')\n",
    "            print(f'---Final hidden state {hidden_state.shape} \\n')\n",
    "            print(f'---output of the last layer at each time step {output.shape} \\n')\n",
    "            print(f'---Final output shape {out.shape} \\n')\n",
    "\n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0e092b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:22:05.426012Z",
     "iopub.status.busy": "2022-03-17T18:22:05.424853Z",
     "iopub.status.idle": "2022-03-17T18:22:05.427142Z",
     "shell.execute_reply": "2022-03-17T18:22:05.427548Z",
     "shell.execute_reply.started": "2022-03-17T18:14:26.058212Z"
    },
    "papermill": {
     "duration": 0.271935,
     "end_time": "2022-03-17T18:22:05.427693",
     "exception": false,
     "start_time": "2022-03-17T18:22:05.155758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_lengths = train_data['glove_index'].apply(lambda x: len(x))\n",
    "train_text = train_data['glove_index']\n",
    "target_train = train_data['target']\n",
    "train_id = train_data['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edb27440",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:22:05.910000Z",
     "iopub.status.busy": "2022-03-17T18:22:05.909283Z",
     "iopub.status.idle": "2022-03-17T18:22:05.911744Z",
     "shell.execute_reply": "2022-03-17T18:22:05.911317Z",
     "shell.execute_reply.started": "2022-03-17T18:14:26.581795Z"
    },
    "papermill": {
     "duration": 0.245413,
     "end_time": "2022-03-17T18:22:05.911847",
     "exception": false,
     "start_time": "2022-03-17T18:22:05.666434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "x_train = pd.DataFrame({'glove_index' : train_data['glove_index']\n",
    "                            , 'train_id': train_id\n",
    "                            , 'train_lengths': train_lengths\n",
    "                            })\n",
    "y_train = train_data['target']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4015037b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:22:06.423769Z",
     "iopub.status.busy": "2022-03-17T18:22:06.422971Z",
     "iopub.status.idle": "2022-03-17T18:22:06.424996Z",
     "shell.execute_reply": "2022-03-17T18:22:06.425451Z",
     "shell.execute_reply.started": "2022-03-17T18:14:27.324040Z"
    },
    "papermill": {
     "duration": 0.260313,
     "end_time": "2022-03-17T18:22:06.425586",
     "exception": false,
     "start_time": "2022-03-17T18:22:06.165273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sf = StratifiedKFold(n_splits = 5, random_state = 3, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "988614bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:22:06.918154Z",
     "iopub.status.busy": "2022-03-17T18:22:06.917337Z",
     "iopub.status.idle": "2022-03-17T18:22:06.919098Z",
     "shell.execute_reply": "2022-03-17T18:22:06.919603Z",
     "shell.execute_reply.started": "2022-03-17T18:14:27.724223Z"
    },
    "papermill": {
     "duration": 0.254191,
     "end_time": "2022-03-17T18:22:06.919728",
     "exception": false,
     "start_time": "2022-03-17T18:22:06.665537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training(model, train, train_target, valid, valid_target, num_epochs, validate = True, learning_rate = 0.005):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    print('Training...')\n",
    "    \n",
    "    #for fold, (train_ind, val_ind) in enumerate(sf.split(x_train, y_train)):\n",
    "    \n",
    "    train_text, train_lengths, train_id, target_train = train['glove_index'].reset_index(drop = True), train['train_lengths'].reset_index(drop = True), train['train_id'].reset_index(drop = True), train_target.reset_index(drop = True)\n",
    "    \n",
    "    train_collate = Collator(percentile = 100)\n",
    "    train_dataset = TextDatset(train_text, train_lengths, train_id, target_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 64, collate_fn = train_collate)\n",
    "    \n",
    "    valid_text, valid_lengths, valid_id, valid_target = valid['glove_index'].reset_index(drop = True), valid['train_lengths'].reset_index(drop = True), valid['train_id'].reset_index(drop = True), valid_target.reset_index(drop = True)\n",
    "\n",
    "\n",
    "    valid_dataset = TextDatset(valid_text, valid_lengths, valid_id, valid_target)\n",
    "    valid_collate = Collator(percentile = 100, test = False)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = 64, collate_fn = valid_collate)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        acc_train = 0\n",
    "        loss_train = 0\n",
    "        model.train()\n",
    "        for batch, (data, target, data_id) in enumerate(train_loader):\n",
    "\n",
    "            pred = model(data, msg = False)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(pred, target)\n",
    "\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += criterion(pred, torch.tensor(target, dtype = torch.long))\n",
    "            acc_train += accuracy(pred, target)\n",
    "\n",
    "        \n",
    "        print(f'Epoch {epoch+1}  |  loss {loss_train / batch}  |  accuracy {acc_train / batch} | {batch}')\n",
    "        \n",
    "#         model.eval()\n",
    "        \n",
    "    if validate:\n",
    "#             print('Validating....')\n",
    "        val_loss = 0\n",
    "        val_acc = 0 \n",
    "\n",
    "        for batch, (data, target, data_id) in enumerate(valid_loader):\n",
    "\n",
    "            pred = model(data, msg = False).detach()\n",
    "            val_loss += criterion(pred, torch.tensor(target, dtype = torch.long))\n",
    "            val_acc += accuracy(pred, target)\n",
    "\n",
    "        \n",
    "    print(f'Validating...')\n",
    "    print(f'validation loss {val_loss / batch}  |  accuracy {val_acc / batch} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2164e7ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:22:07.422967Z",
     "iopub.status.busy": "2022-03-17T18:22:07.422011Z",
     "iopub.status.idle": "2022-03-17T18:22:07.791999Z",
     "shell.execute_reply": "2022-03-17T18:22:07.791440Z",
     "shell.execute_reply.started": "2022-03-17T18:14:28.400661Z"
    },
    "papermill": {
     "duration": 0.622973,
     "end_time": "2022-03-17T18:22:07.792137",
     "exception": false,
     "start_time": "2022-03-17T18:22:07.169164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of hidden layers and inputs before passing to the model \n",
      "\n",
      "...... \n",
      "\n",
      "---Input shape torch.Size([64, 13]) \n",
      "\n",
      "---hidden layer shape torch.Size([6, 64, 256]) \n",
      "\n",
      "The dimension of final hidden state and output state after passing to the model \n",
      "\n",
      "....... \n",
      "\n",
      "---Input shape torch.Size([64, 13, 200]) \n",
      "\n",
      "---Final hidden state torch.Size([6, 64, 256]) \n",
      "\n",
      "---output of the last layer at each time step torch.Size([64, 13, 512]) \n",
      "\n",
      "---Final output shape torch.Size([64, 2]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0078,  0.0374],\n",
       "        [-0.0132,  0.0364],\n",
       "        [-0.0040,  0.0349],\n",
       "        [-0.0078,  0.0000],\n",
       "        [-0.0078,  0.0291],\n",
       "        [-0.0075,  0.0391],\n",
       "        [-0.0000,  0.0362],\n",
       "        [-0.0105,  0.0351],\n",
       "        [-0.0101,  0.0345],\n",
       "        [-0.0112,  0.0364],\n",
       "        [-0.0091,  0.0384],\n",
       "        [-0.0071,  0.0364],\n",
       "        [-0.0079,  0.0406],\n",
       "        [-0.0080,  0.0375],\n",
       "        [-0.0091,  0.0398],\n",
       "        [-0.0107,  0.0346],\n",
       "        [-0.0105,  0.0377],\n",
       "        [-0.0112,  0.0376],\n",
       "        [-0.0112,  0.0371],\n",
       "        [-0.0111,  0.0370],\n",
       "        [-0.0111,  0.0370],\n",
       "        [-0.0096,  0.0361],\n",
       "        [-0.0100,  0.0365],\n",
       "        [-0.0112,  0.0363],\n",
       "        [-0.0000,  0.0370],\n",
       "        [-0.0095,  0.0000],\n",
       "        [-0.0105,  0.0000],\n",
       "        [-0.0108,  0.0000],\n",
       "        [-0.0110,  0.0369],\n",
       "        [-0.0103,  0.0357],\n",
       "        [-0.0111,  0.0000],\n",
       "        [-0.0092,  0.0385],\n",
       "        [-0.0089,  0.0357],\n",
       "        [-0.0080,  0.0383],\n",
       "        [-0.0000,  0.0000],\n",
       "        [-0.0000,  0.0374],\n",
       "        [-0.0040,  0.0374],\n",
       "        [-0.0000,  0.0359],\n",
       "        [-0.0063,  0.0000],\n",
       "        [-0.0100,  0.0371],\n",
       "        [-0.0097,  0.0359],\n",
       "        [-0.0068,  0.0359],\n",
       "        [-0.0000,  0.0000],\n",
       "        [-0.0086,  0.0371],\n",
       "        [-0.0093,  0.0365],\n",
       "        [-0.0059,  0.0406],\n",
       "        [-0.0000,  0.0000],\n",
       "        [-0.0083,  0.0368],\n",
       "        [-0.0097,  0.0359],\n",
       "        [-0.0070,  0.0000],\n",
       "        [-0.0089,  0.0344],\n",
       "        [-0.0073,  0.0357],\n",
       "        [-0.0139,  0.0293],\n",
       "        [-0.0091,  0.0344],\n",
       "        [-0.0086,  0.0358],\n",
       "        [ 0.0004,  0.0423],\n",
       "        [-0.0066,  0.0347],\n",
       "        [-0.0071,  0.0335],\n",
       "        [-0.0000,  0.0332],\n",
       "        [-0.0066,  0.0388],\n",
       "        [-0.0041,  0.0321],\n",
       "        [-0.0047,  0.0363],\n",
       "        [-0.0071,  0.0401],\n",
       "        [-0.0000,  0.0334]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_collate = Collator(percentile = 100)\n",
    "train_dataset = TextDatset(train_text, train_lengths, train_id, target_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 64, collate_fn = train_collate)\n",
    "data, target, data_id = next(iter(train_loader))\n",
    "\n",
    "model = textRNN(input_size, hidden_size, num_layers, num_classes, batch_size, model_type = 'LSTM')\n",
    "model(data, msg = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed0db04e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:22:08.288660Z",
     "iopub.status.busy": "2022-03-17T18:22:08.287694Z",
     "iopub.status.idle": "2022-03-17T18:22:08.290349Z",
     "shell.execute_reply": "2022-03-17T18:22:08.289712Z",
     "shell.execute_reply.started": "2022-03-17T18:14:29.588880Z"
    },
    "papermill": {
     "duration": 0.250882,
     "end_time": "2022-03-17T18:22:08.290517",
     "exception": false,
     "start_time": "2022-03-17T18:22:08.039635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_id = list(data_id)\n",
    "# np.array([train_data[train_data['id'].isin(data_id)].glove_index.apply(lambda x: len(x))]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92573e09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:22:08.782828Z",
     "iopub.status.busy": "2022-03-17T18:22:08.782277Z",
     "iopub.status.idle": "2022-03-17T18:28:30.681121Z",
     "shell.execute_reply": "2022-03-17T18:28:30.681762Z"
    },
    "papermill": {
     "duration": 382.148657,
     "end_time": "2022-03-17T18:28:30.681975",
     "exception": false,
     "start_time": "2022-03-17T18:22:08.533318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold1 \n",
      "\n",
      "Training...\n",
      "Epoch 1  |  loss 0.6376305222511292  |  accuracy 0.6891776315789473 | 95\n",
      "Epoch 2  |  loss 0.5305585265159607  |  accuracy 0.7728947368421052 | 95\n",
      "Epoch 3  |  loss 0.4933263659477234  |  accuracy 0.7931907894736843 | 95\n",
      "Validating...\n",
      "validation loss 0.5136802792549133  |  accuracy 0.8141650682011935 \n",
      "\n",
      "Fold2 \n",
      "\n",
      "Training...\n",
      "Epoch 1  |  loss 0.6294810771942139  |  accuracy 0.6954934210526317 | 95\n",
      "Epoch 2  |  loss 0.549085259437561  |  accuracy 0.7626973684210526 | 95\n",
      "Epoch 3  |  loss 0.4980575442314148  |  accuracy 0.7908881578947369 | 95\n",
      "Validating...\n",
      "validation loss 0.5315901637077332  |  accuracy 0.802949168797954 \n",
      "\n",
      "Fold3 \n",
      "\n",
      "Training...\n",
      "Epoch 1  |  loss 0.6233391165733337  |  accuracy 0.7126973684210526 | 95\n",
      "Epoch 2  |  loss 0.538622260093689  |  accuracy 0.765921052631579 | 95\n",
      "Epoch 3  |  loss 0.49883612990379333  |  accuracy 0.7890789473684211 | 95\n",
      "Validating...\n",
      "validation loss 0.506109893321991  |  accuracy 0.816389599317988 \n",
      "\n",
      "Fold4 \n",
      "\n",
      "Training...\n",
      "Epoch 1  |  loss 0.6079588532447815  |  accuracy 0.716866028708134 | 95\n",
      "Epoch 2  |  loss 0.528107762336731  |  accuracy 0.7811752392344499 | 95\n",
      "Epoch 3  |  loss 0.5051453113555908  |  accuracy 0.7973983253588517 | 95\n",
      "Validating...\n",
      "validation loss 0.5191603302955627  |  accuracy 0.813804347826087 \n",
      "\n",
      "Fold5 \n",
      "\n",
      "Training...\n",
      "Epoch 1  |  loss 0.6200721263885498  |  accuracy 0.7100627990430621 | 95\n",
      "Epoch 2  |  loss 0.5238893628120422  |  accuracy 0.7796351674641148 | 95\n",
      "Epoch 3  |  loss 0.4919264018535614  |  accuracy 0.7958881578947369 | 95\n",
      "Validating...\n",
      "validation loss 0.5378467440605164  |  accuracy 0.8101902173913043 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KFOLD validation \n",
    "\n",
    "for fold, (train_ind, val_ind) in enumerate(sf.split(x_train, y_train)):\n",
    "    train, train_target = x_train.iloc[train_ind, :], y_train[train_ind]\n",
    "    valid, valid_target = x_train.iloc[val_ind, :], y_train[val_ind]\n",
    "    \n",
    "    print(f'Fold{fold + 1} \\n')\n",
    "    \n",
    "    \n",
    "    model = textRNN(input_size, hidden_size, num_layers, num_classes, batch_size, model_type = 'LSTM')\n",
    "    \n",
    "    training(model, train, train_target, valid, valid_target, num_epochs, validate = True, learning_rate = 0.001)\n",
    "                                    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7494cf90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:28:31.198838Z",
     "iopub.status.busy": "2022-03-17T18:28:31.197930Z",
     "iopub.status.idle": "2022-03-17T18:28:35.765128Z",
     "shell.execute_reply": "2022-03-17T18:28:35.765621Z"
    },
    "papermill": {
     "duration": 4.82975,
     "end_time": "2022-03-17T18:28:35.765791",
     "exception": false,
     "start_time": "2022-03-17T18:28:30.936041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#testing\n",
    "\n",
    "test_lengths = test_data['glove_index'].apply(lambda x: len(x))\n",
    "test_text = test_data['glove_index']\n",
    "test_id = test_data['id']\n",
    "\n",
    "\n",
    "test_dataset = TextDatset(test_text, test_lengths, test_id)\n",
    "test_collate = Collator(percentile = 100, test = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 64, collate_fn = test_collate)\n",
    "\n",
    "\n",
    "\n",
    "ids = []\n",
    "out = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for batch, (data, text_id) in enumerate(test_loader):\n",
    "    test_out = model(data, msg = False)\n",
    "    ids.extend(text_id)\n",
    "    out.extend(test_out.max(dim = 1)[1].detach().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05a663c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T18:28:36.288682Z",
     "iopub.status.busy": "2022-03-17T18:28:36.287937Z",
     "iopub.status.idle": "2022-03-17T18:28:36.299896Z",
     "shell.execute_reply": "2022-03-17T18:28:36.299428Z",
     "shell.execute_reply.started": "2022-03-11T10:25:55.366158Z"
    },
    "papermill": {
     "duration": 0.267652,
     "end_time": "2022-03-17T18:28:36.300012",
     "exception": false,
     "start_time": "2022-03-17T18:28:36.032360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_file = pd.DataFrame({'id' : ids, 'target': out })\n",
    "sub_file.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 642.7808,
   "end_time": "2022-03-17T18:28:39.508549",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-17T18:17:56.727749",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
